from __future__ import annotations

import asyncio
import logging
import random
from dataclasses import dataclass
from typing import Optional, Dict, Any

import httpx
from tenacity import retry, stop_after_attempt, wait_exponential

log = logging.getLogger(__name__)


@dataclass
class LLMRequest:
    prompt: str
    chat_context: Optional[str] = None
    temperature: float = 0.7
    max_tokens: int = 256
    system_prompt: Optional[str] = None


class LLMClient:
    """Base class for LLM clients"""
    
    def generate(self, req: LLMRequest) -> str:
        """Synchronous generation"""
        raise NotImplementedError
    
    async def generate_async(self, req: LLMRequest) -> str:
        """Asynchronous generation"""
        # Default implementation runs sync version in thread pool
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.generate, req)


class StubLLM(LLMClient):
    """Enhanced stub that generates more contextual responses"""

    def __init__(self):
        self.response_templates = {
            "greeting": [
                "ÐŸÑ€Ð¸Ð²ÐµÑ‚! ÐšÐ°Ðº Ð´ÐµÐ»Ð°? â˜ºï¸",
                "Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ! Ð Ð°Ð´Ð° Ð²Ð°Ñ Ð²Ð¸Ð´ÐµÑ‚ÑŒ ðŸ™",
                "ÐŸÑ€Ð¸Ð²ÐµÑ‚Ð¸Ðº! ÐšÐ°ÐºÐ¾Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ? âœ¨"
            ],
            "meditation": [
                "Ðž, Ð¼ÐµÐ´Ð¸Ñ‚Ð°Ñ†Ð¸Ñ - ÑÑ‚Ð¾ Ð¿Ñ€ÐµÐºÑ€Ð°ÑÐ½Ð¾! Ð¯ ÑÐ°Ð¼Ð° Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÑƒÑŽ ÐºÐ°Ð¶Ð´Ð¾Ðµ ÑƒÑ‚Ñ€Ð¾ ðŸ§˜â€â™€ï¸",
                "ÐœÐµÐ´Ð¸Ñ‚Ð°Ñ†Ð¸Ñ Ñ‚Ð°Ðº Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð½Ð°Ð¹Ñ‚Ð¸ Ð±Ð°Ð»Ð°Ð½Ñ Ð² Ð¶Ð¸Ð·Ð½Ð¸ âœ¨",
                "ÐžÐ±Ð¾Ð¶Ð°ÑŽ Ð¼ÐµÐ´Ð¸Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ, Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ Ð½Ð° Ñ€Ð°ÑÑÐ²ÐµÑ‚Ðµ ðŸŒ…"
            ],
            "yoga": [
                "Ð™Ð¾Ð³Ð° - Ð¼Ð¾Ñ ÑÑ‚Ñ€Ð°ÑÑ‚ÑŒ! ÐšÐ°ÐºÐ¸Ðµ Ð°ÑÐ°Ð½Ñ‹ Ð»ÑŽÐ±Ð¸Ñ‚Ðµ? ðŸ§˜â€â™€ï¸",
                "ÐŸÑ€Ð°ÐºÑ‚Ð¸ÐºÑƒÑŽ Ð¹Ð¾Ð³Ñƒ ÑƒÐ¶Ðµ 5 Ð»ÐµÑ‚, ÑÑ‚Ð¾ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ð»Ð¾ Ð¼Ð¾ÑŽ Ð¶Ð¸Ð·Ð½ÑŒ ðŸ™",
                "Ð™Ð¾Ð³Ð° - Ð¿ÑƒÑ‚ÑŒ Ðº Ð³Ð°Ñ€Ð¼Ð¾Ð½Ð¸Ð¸ Ð´ÑƒÑˆÐ¸ Ð¸ Ñ‚ÐµÐ»Ð° âœ¨"
            ],
            "support": [
                "Ð’ÑÐµ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð½Ð°Ð»Ð°Ð´Ð¸Ñ‚ÑÑ, Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð² ÑÐµÐ±Ñ! ðŸ’ª",
                "ÐŸÐ¾Ð½Ð¸Ð¼Ð°ÑŽ Ð²Ð°Ñ, Ð¸Ð½Ð¾Ð³Ð´Ð° Ð±Ñ‹Ð²Ð°ÐµÑ‚ Ð½ÐµÐ¿Ñ€Ð¾ÑÑ‚Ð¾ ðŸ¤—",
                "Ð’Ñ‹ ÑÐ¿Ñ€Ð°Ð²Ð¸Ñ‚ÐµÑÑŒ, Ñ Ð² Ð²Ð°Ñ Ð²ÐµÑ€ÑŽ! â­"
            ],
            "general": [
                "Ð˜Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð°Ñ Ð¼Ñ‹ÑÐ»ÑŒ, Ñ€Ð°ÑÑÐºÐ°Ð¶Ð¸Ñ‚Ðµ Ð±Ð¾Ð»ÑŒÑˆÐµ ðŸ¤”",
                "Ð”Ð°, Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð° Ñ Ð²Ð°Ð¼Ð¸!",
                "ÐšÐ°Ðº Ð·Ð´Ð¾Ñ€Ð¾Ð²Ð¾, Ñ‡Ñ‚Ð¾ Ð²Ñ‹ ÑÑ‚Ð¾ Ð¿Ð¾Ð´Ð¼ÐµÑ‚Ð¸Ð»Ð¸ âœ¨",
                "Ðž, Ð½Ð¸ÐºÐ¾Ð³Ð´Ð° Ð¾Ð± ÑÑ‚Ð¾Ð¼ Ð½Ðµ Ð´ÑƒÐ¼Ð°Ð»Ð°! Ð›ÑŽÐ±Ð¾Ð¿Ñ‹Ñ‚Ð½Ð¾ ðŸ’«"
            ]
        }

    def generate(self, req: LLMRequest) -> str:
        prompt_lower = req.prompt.lower()
        
        # Simple keyword matching for context
        if any(word in prompt_lower for word in ["Ð¿Ñ€Ð¸Ð²ÐµÑ‚", "Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²", "Ð´Ð¾Ð±Ñ€Ñ‹Ð¹"]):
            responses = self.response_templates["greeting"]
        elif any(word in prompt_lower for word in ["Ð¼ÐµÐ´Ð¸Ñ‚Ð°Ñ†", "Ð¾ÑÐ¾Ð·Ð½Ð°Ð½", "mindful"]):
            responses = self.response_templates["meditation"]
        elif any(word in prompt_lower for word in ["Ð¹Ð¾Ð³Ð°", "Ð°ÑÐ°Ð½", "Ð¿Ñ€Ð°Ð½Ð°ÑÐ¼"]):
            responses = self.response_templates["yoga"]
        elif any(word in prompt_lower for word in ["Ñ‚Ñ€ÑƒÐ´Ð½Ð¾", "ÑÐ»Ð¾Ð¶Ð½Ð¾", "Ð¿Ð¾Ð¼Ð¾Ð³", "Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶"]):
            responses = self.response_templates["support"]
        else:
            responses = self.response_templates["general"]
        
        base_response = random.choice(responses)
        
        # Small chance to add promotion (will be controlled by policy)
        # Promotion logic is handled at higher level now
        
        return base_response


class OpenAIClient(LLMClient):
    """OpenAI API client with retry logic"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini", 
                 temperature: float = 0.7, base_url: Optional[str] = None):
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.base_url = base_url or "https://api.openai.com/v1"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))
    async def generate_async(self, req: LLMRequest) -> str:
        """Generate response using OpenAI API"""
        async with httpx.AsyncClient(timeout=30.0) as client:
            messages = []
            
            if req.system_prompt:
                messages.append({"role": "system", "content": req.system_prompt})
            
            if req.chat_context:
                messages.append({"role": "user", "content": req.chat_context})
            
            messages.append({"role": "user", "content": req.prompt})
            
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": req.temperature or self.temperature,
                "max_tokens": req.max_tokens,
                "presence_penalty": 0.6,  # Reduce repetition
                "frequency_penalty": 0.3
            }
            
            try:
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload
                )
                response.raise_for_status()
                
                data = response.json()
                return data["choices"][0]["message"]["content"].strip()
                
            except httpx.HTTPStatusError as e:
                log.error(f"OpenAI API error: {e.response.status_code} - {e.response.text}")
                raise
            except Exception as e:
                log.error(f"OpenAI API error: {e}")
                raise
    
    def generate(self, req: LLMRequest) -> str:
        """Synchronous wrapper"""
        return asyncio.run(self.generate_async(req))


class AnthropicClient(LLMClient):
    """Anthropic Claude API client"""
    
    def __init__(self, api_key: str, model: str = "claude-3-haiku-20240307",
                 temperature: float = 0.7):
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.headers = {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json"
        }
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))
    async def generate_async(self, req: LLMRequest) -> str:
        """Generate response using Anthropic API"""
        async with httpx.AsyncClient(timeout=30.0) as client:
            system_prompt = req.system_prompt or "You are a helpful assistant."
            
            user_content = req.prompt
            if req.chat_context:
                user_content = f"{req.chat_context}\n\n{req.prompt}"
            
            payload = {
                "model": self.model,
                "system": system_prompt,
                "messages": [{"role": "user", "content": user_content}],
                "temperature": req.temperature or self.temperature,
                "max_tokens": req.max_tokens
            }
            
            try:
                response = await client.post(
                    "https://api.anthropic.com/v1/messages",
                    headers=self.headers,
                    json=payload
                )
                response.raise_for_status()
                
                data = response.json()
                return data["content"][0]["text"].strip()
                
            except httpx.HTTPStatusError as e:
                log.error(f"Anthropic API error: {e.response.status_code} - {e.response.text}")
                raise
            except Exception as e:
                log.error(f"Anthropic API error: {e}")
                raise
    
    def generate(self, req: LLMRequest) -> str:
        """Synchronous wrapper"""
        return asyncio.run(self.generate_async(req))


class GoogleGeminiClient(LLMClient):
    """Google Gemini API client"""
    
    def __init__(self, api_key: str, model: str = "gemini-1.5-flash",
                 temperature: float = 0.7):
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))
    async def generate_async(self, req: LLMRequest) -> str:
        """Generate response using Google Gemini API"""
        async with httpx.AsyncClient(timeout=30.0) as client:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model}:generateContent"
            
            prompt = req.prompt
            if req.system_prompt:
                prompt = f"{req.system_prompt}\n\n{prompt}"
            if req.chat_context:
                prompt = f"{req.chat_context}\n\n{prompt}"
            
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {
                    "temperature": req.temperature or self.temperature,
                    "maxOutputTokens": req.max_tokens,
                    "topP": 0.8,
                    "topK": 40
                }
            }
            
            try:
                response = await client.post(
                    url,
                    params={"key": self.api_key},
                    json=payload
                )
                response.raise_for_status()
                
                data = response.json()
                return data["candidates"][0]["content"]["parts"][0]["text"].strip()
                
            except httpx.HTTPStatusError as e:
                log.error(f"Google Gemini API error: {e.response.status_code} - {e.response.text}")
                raise
            except Exception as e:
                log.error(f"Google Gemini API error: {e}")
                raise
    
    def generate(self, req: LLMRequest) -> str:
        """Synchronous wrapper"""
        return asyncio.run(self.generate_async(req))


def create_llm_client(config: Dict[str, Any]) -> LLMClient:
    """Factory function to create appropriate LLM client"""
    provider = config.get("provider", "stub").lower()
    
    if provider == "stub":
        return StubLLM()
    
    elif provider == "openai":
        if not config.get("api_key"):
            log.warning("OpenAI API key not provided, falling back to StubLLM")
            return StubLLM()
        
        return OpenAIClient(
            api_key=config["api_key"],
            model=config.get("model", "gpt-4o-mini"),
            temperature=config.get("temperature", 0.7),
            base_url=config.get("base_url")
        )
    
    elif provider == "anthropic":
        if not config.get("api_key"):
            log.warning("Anthropic API key not provided, falling back to StubLLM")
            return StubLLM()
        
        return AnthropicClient(
            api_key=config["api_key"],
            model=config.get("model", "claude-3-haiku-20240307"),
            temperature=config.get("temperature", 0.7)
        )
    
    elif provider == "google" or provider == "gemini":
        if not config.get("api_key"):
            log.warning("Google API key not provided, falling back to StubLLM")
            return StubLLM()
        
        return GoogleGeminiClient(
            api_key=config["api_key"],
            model=config.get("model", "gemini-1.5-flash"),
            temperature=config.get("temperature", 0.7)
        )
    
    else:
        log.warning(f"Unknown LLM provider: {provider}, falling back to StubLLM")
        return StubLLM()

